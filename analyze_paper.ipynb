{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to test and compare different prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def upload_analyze_papers(food, hazard, pdfs, API_Key, question='default'):\n",
    "    all_results = pd.DataFrame(columns=['filename', 'foodname','species','othername', 'hazard', 'quote', 'location', 'pos/neg', 'how','howquote']) \n",
    "\n",
    "    \n",
    "    # Load papers into chatpdf\n",
    "    # dummy loop to only upload 12/minute\n",
    "    uploads = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for file in pdfs:\n",
    "        # Check if we've reached the upload limit\n",
    "        if uploads >= 12:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            if elapsed_time < 60:\n",
    "                # If uploads exceed the limit in less than a minute, wait for the remaining time\n",
    "                wait_time = 60 - elapsed_time\n",
    "                print(f'Upload limit reached. Waiting for {wait_time:.2f} seconds...')\n",
    "                time.sleep(wait_time)\n",
    "            uploads = 0  # Reset the counter and start a new minute\n",
    "            start_time = time.time()\n",
    "\n",
    "        print(f'loading file \"{os.path.basename(file)}\" ')\n",
    "        files = [('file', ('file', open(file, 'rb'), 'application/octet-stream'))]\n",
    "        headers = {'x-api-key': API_Key}\n",
    "        response = requests.post('https://api.chatpdf.com/v1/sources/add-file', headers=headers, files=files)\n",
    "        \n",
    "        # Update the upload counter\n",
    "        uploads += 1\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            sID = response.json()['sourceId']\n",
    "        else:\n",
    "            print('Status:', response.status_code)\n",
    "            print('Error:', response.text)\n",
    "            continue\n",
    "\n",
    "        # 'Read' papers in chatpdf\n",
    "        headers = {\n",
    "            'x-api-key': API_Key,\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        print('analyzing file...')\n",
    "        if question == 'default':\n",
    "            question = f'1a) provide a quote from the text about how {hazard} impacts {food}? 2a) in what region is this study 3a) does {hazard} negatively or positively impact {food} (reply only negatvie/positive) 4a) would you classify the impact on {food} as direct physiology, indirect abiotic, or indirect biotic?  5) can you give a quote providing an example of this?'\n",
    "\n",
    "        data = {\n",
    "            'sourceId': sID,\n",
    "            'messages': [\n",
    "                {\n",
    "                    'role': \"user\",\n",
    "                    'content': question,\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        response = requests.post(\n",
    "            'https://api.chatpdf.com/v1/chats/message', headers=headers, json=data)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            print('result:', response.json()['content'])\n",
    "            results = response.json()['content']\n",
    "            segments = results.split('a)')  # Split the text using regex\n",
    "            if len(segments) >= 5:\n",
    "                quote = segments[1].strip()\n",
    "                location = segments[2].strip()\n",
    "                posneg = segments[3].strip()\n",
    "                how = segments[4].strip()\n",
    "                howquote = segments[5].strip()\n",
    "\n",
    "                # Create a row dictionary for this PDF\n",
    "                row_data = {\n",
    "                    'filename': os.path.basename(file),\n",
    "                    list: food,\n",
    "                    'hazard': hazard,\n",
    "                    'quote': quote,\n",
    "                    'location': location,\n",
    "                    'pos/neg': posneg,\n",
    "                    'how': how,\n",
    "                    'howquote': howquote\n",
    "                }\n",
    "\n",
    "                # Append the row data to the results\n",
    "                all_results = pd.concat([all_results, pd.DataFrame([row_data])], ignore_index=True)\n",
    "        else:\n",
    "            print('Status:', response.status_code)\n",
    "            print('Error:', response.text)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "foods = ['Bay leaf','Bignose unicornfish','Button mushroom','Centropristis philadelphica','Chinese softshell turtle','Corica soborna','Ganges river sprat','Grasshopper','Large yellow croaker','Larimichthys croceus','Leucoraja erinacea','Little skate','mealworm','Naso vlamingii','Orange-spotted grouper','Ordways brotula','Pusa hispida','Ringed seal','Rock sea bass','Tenebrio molitor','Trionyx sinensis','Triticum aestivum','wheat']\n",
    "hazards = ['atmospheric CO2 increases','ocean acidification','ozone','storms','warming','precipitation','precipitation','fires','warming','warming','ocean acidification','ocean acidification','warming','ocean acidification','natural cover change','floods','heatwaves','Heatwaves','storms','warming','warming','drought','drought']\n",
    "API_Key = 'sec_6LIDxgLBHBqmhkVam818PYYXqervcPSX' #make a user and get api key from https://www.chatpdf.com/docs/api/backend\n",
    "\n",
    "question  = 'XX'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do for prompt test\n",
    "- load papers\n",
    "- get comporable output as human df\n",
    "- make list/dict/df with question and dfID\n",
    "\n",
    "\n",
    "To do for big tool\n",
    "- find effective way to combine bibtxt csv and/or pdfs into groupings that make sense\n",
    "- either append resutls here or on pypaperbot csv\n",
    "- subset run to 'smaller' batches \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
