{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQXL6WvaRjgH"
   },
   "source": [
    "## function to download and analyse papers from google scholar\n",
    "- This function will do a google scholar search based on a query (food + hazard)\n",
    "- It will attempt to download the first 20 results (mostly through scihub)\n",
    "- Then ChatPDF will check through each pdf and answer your question and find quotes within each pdf\n",
    "\n",
    "You need\n",
    "1. To install PyPaperBot (you can do this in anaconda terminal by typing *pip install PyPaperBot*\n",
    "2. A chatPDF API Key. you can get this by making a chatPDF user and clicking *My Account* on the ChatPDF API website https://www.chatpdf.com/docs/api/backend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xutX7eZaM8C"
   },
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import glob\n",
    "import PyPaperBot\n",
    "from PyPaperBot import __main__ as p\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS to download and review the papers\n",
    "\n",
    "def download_papers(food, hazard, scholar_pages=[1,2], scholar_results=20, skip_if_folder_exists = True):\n",
    "  ## ----------------------------------- ###\n",
    "    # specify download folder\n",
    "    folder_name = f'{food}_{hazard}'\n",
    "    dwn_dir = os.path.join(os.getcwd(), folder_name)\n",
    "    if os.path.exists(dwn_dir):\n",
    "        pdfs = glob.glob(os.path.join(dwn_dir, '*.pdf'))\n",
    "        if len(pdfs) > scholar_results and skip_if_folder_exists\n",
    "            print('found %d papers in the folder \"%s\"' %(len(pdfs), folder_name))\n",
    "            return pdfs\n",
    "    else: # not os.path.exists(dwn_dir)\n",
    "        os.mkdir(dwn_dir)\n",
    "        # make query:\n",
    "        query = f'{food}+{hazard}'\n",
    "        #download papers from google scholar\n",
    "        p.start(query=query, scholar_pages=scholar_pages, scholar_results=scholar_results, dwn_dir=dwn_dir, proxy=[])\n",
    "\n",
    "        pdfs = glob.glob(os.path.join(dwn_dir, '*.pdf'))\n",
    "        print('downloaded %d of %d papers' %(len(pdfs), scholar_results))\n",
    "    return pdfs\n",
    "\n",
    "\n",
    "def upload_analyze_papers(food, hazard, pdfs, API_Key, question='default'):\n",
    "    all_results = pd.DataFrame(columns=['filename', 'food', 'hazard', 'quote', 'location', 'pos/neg', 'how']) \n",
    "\n",
    "    # Load papers into chatpdf\n",
    "    # dummy loop to only upload 12/minute\n",
    "    uploads = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for file in pdfs:\n",
    "        # Check if we've reached the upload limit\n",
    "        if uploads >= 12:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            if elapsed_time < 60:\n",
    "                # If uploads exceed the limit in less than a minute, wait for the remaining time\n",
    "                wait_time = 60 - elapsed_time\n",
    "                print(f'Upload limit reached. Waiting for {wait_time:.2f} seconds...')\n",
    "                time.sleep(wait_time)\n",
    "            uploads = 0  # Reset the counter and start a new minute\n",
    "            start_time = time.time()\n",
    "\n",
    "        print(f'loading file \"{os.path.basename(file)}\" ')\n",
    "        files = [('file', ('file', open(file, 'rb'), 'application/octet-stream'))]\n",
    "        headers = {'x-api-key': API_Key}\n",
    "        response = requests.post('https://api.chatpdf.com/v1/sources/add-file', headers=headers, files=files)\n",
    "        \n",
    "        # Update the upload counter\n",
    "        uploads += 1\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            sID = response.json()['sourceId']\n",
    "        else:\n",
    "            print('Status:', response.status_code)\n",
    "            print('Error:', response.text)\n",
    "            continue\n",
    "\n",
    "        # 'Read' papers in chatpdf\n",
    "        headers = {\n",
    "            'x-api-key': API_Key,\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        print('analyzing file...')\n",
    "        if question == 'default':\n",
    "            question = f'1a) provide a quote from the text about how {hazard} impacts {food}? 2a) in what region is this study 3a) does {hazard} negatively or positively impact {food} (reply only negatvie/positive) 4a) exactly how is {food} impacted?'\n",
    "\n",
    "        data = {\n",
    "            'sourceId': sID,\n",
    "            'messages': [\n",
    "                {\n",
    "                    'role': \"user\",\n",
    "                    'content': question,\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        response = requests.post(\n",
    "            'https://api.chatpdf.com/v1/chats/message', headers=headers, json=data)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            print('result:', response.json()['content'])\n",
    "            results = response.json()['content']\n",
    "            segments = results.split('a)')  # Split the text using regex\n",
    "            if len(segments) >= 4:\n",
    "                quote = segments[0].strip()\n",
    "                location = segments[1].strip()\n",
    "                page = segments[2].strip()\n",
    "                paragraph = segments[3].strip()\n",
    "\n",
    "                # Create a row dictionary for this PDF\n",
    "                row_data = {\n",
    "                    'filename': os.path.basename(file),\n",
    "                    'food': food,\n",
    "                    'hazard': hazard,\n",
    "                    'quote': quote,\n",
    "                    'location': location,\n",
    "                    'pos/neg': page,\n",
    "                    'how': paragraph\n",
    "                }\n",
    "\n",
    "                # Append the row data to the results\n",
    "                all_results = pd.concat([all_results, pd.DataFrame([row_data])], ignore_index=True)\n",
    "        else:\n",
    "            print('Status:', response.status_code)\n",
    "            print('Error:', response.text)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "def download_read_export(food, hazard, API_Key, scholar_pages=[1,2], scholar_results=20, question='default', skip_if_folder_exists = True):\n",
    "    start_time = time.time() #start timer\n",
    "\n",
    "    pdfs = download_papers(food, hazard, scholar_pages, scholar_results, skip_if_folder_exists)\n",
    "    df = upload_analyze_papers(food, hazard,pdfs, API_Key, question='default')\n",
    "    return df\n",
    "\n",
    "    end_time = time.time() #stop timer\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f'Time taken for \"{food}\" and \"{hazard}\":{elapsed_time:.2f} seconds')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Run it\n",
    "\n",
    "foods = ['tuna']#, 'rice', 'tuna', 'apple', 'coffee']\n",
    "hazards = ['heatwave']#, ['drought', 'heatwave', 'warming', 'storm', 'flooding']\n",
    "API_Key = 'sec_6LIDxgLBHBqmhkVam818PYYXqervcPSX' #make a user and get api key from https://www.chatpdf.com/docs/api/backend\n",
    "scholar_pages = [1]\n",
    "scholar_results = 2\n",
    "question  = 'default' #f'1a) provide a quote from the text about how {hazard} impacts {food}? 2a) in what region is this study 3a) does {hazard}Â negatively or positively impact {food} (reply only negatvie/positive) 4a) exactly how is {food} impacted?'\n",
    "\n",
    "# Create an empty dataframe to store the results\n",
    "dfall = pd.DataFrame(columns=['filename', 'food', 'hazard', 'quote', 'location', 'pos/neg', 'how']) ## Specify a question in quotes or use the default: \"How does {hazard} impact {food}? can you provide a quote from the text about this?\"\n",
    "skip_if_folder_exists = True ## dummy variable, if you have the data downloaded already and dont want to re-download it. defualt=True\n",
    "\n",
    "# Create an empty dataframe to store the results\n",
    "#dfall = pd.DataFrame(columns=['food', 'hazard', 'quote', 'location', 'page', 'paragraph'])\n",
    "\n",
    "## Run function across combinations of food and hazard\n",
    "for food in foods:\n",
    "  for hazard in hazards:\n",
    "    df = download_read_export(food, hazard, API_Key, scholar_pages=scholar_pages, scholar_results=scholar_results, question=question, skip_if_folder_exists = True)\n",
    "    dfall = pd.concat([dfall, df], ignore_index=True)\n",
    "\n",
    "# # Print the merged dataframe\n",
    "print(dfall)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UPggkaO1xMZ2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
